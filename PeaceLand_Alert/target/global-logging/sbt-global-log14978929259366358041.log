[0m[[0m[0mdebug[0m] [0m[0m> Exec(run, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0mEvaluating tasks: Compile / run[0m
[0m[[0m[0mdebug[0m] [0m[0mRunning task... Cancel: Signal, check cycles: false, forcegc: true[0m
[0m[[0m[0minfo[0m] [0m[0mcompiling 1 Scala source to /home/julie/Desktop/Data_Engineering_EKANE_NGAN_NZEKET/PeaceLand_Alert/target/scala-2.12/classes ...[0m
[0m[[0m[0minfo[0m] [0m[0mdone compiling[0m
[0m[[0m[31merror[0m] [0m[0morg.apache.spark.sql.AnalysisException: cannot resolve 'array_min(`pscore`)' due to data type mismatch: argument 1 requires array type, however, '`pscore`' is of int type.;[0m
[0m[[0m[31merror[0m] [0m[0m'Filter (array_min(pscore#28) < 0.2)[0m
[0m[[0m[31merror[0m] [0m[0m+- Project [report#23.id AS id#25, report#23.emotion AS emotion#26, report#23.behavior AS behavior#27, report#23.pscore AS pscore#28, report#23.datetime AS datetime#29, report#23.lat AS lat#30, report#23.lon AS lon#31, report#23.words AS words#32][0m
[0m[[0m[31merror[0m] [0m[0m   +- Project [from_json(StructField(id,StringType,true), StructField(emotion,StringType,true), StructField(behavior,StringType,true), StructField(pscore,IntegerType,true), StructField(datetime,StringType,true), StructField(lat,StringType,true), StructField(lon,StringType,true), StructField(words,ArrayType(StringType,true),true), value#21, Some(Europe/Paris)) AS report#23][0m
[0m[[0m[31merror[0m] [0m[0m      +- Project [cast(value#8 as string) AS value#21][0m
[0m[[0m[31merror[0m] [0m[0m         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7a6e0f3c, org.apache.spark.sql.kafka010.KafkaSourceProvider, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@49c37918, [startingOffsets=latest, kafka.bootstrap.servers=localhost:9092, subscribe=test, failOnDataLoss=false], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@43de88f3,org.apache.spark.sql.kafka010.KafkaSourceProvider,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> test, startingOffsets -> latest, failOnDataLoss -> false),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6][0m
[0m[[0m[31merror[0m] [0m[0m[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:161)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:338)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:338)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:210)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:216)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3720)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1596)[0m
[0m[[0m[31merror[0m] [0m[0m	at alert.Streamer$.main(Streamer.scala:60)[0m
[0m[[0m[31merror[0m] [0m[0m	at alert.Streamer.main(Streamer.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.reflect.Method.invoke(Method.java:566)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.spark.sql.AnalysisException: cannot resolve 'array_min(`pscore`)' due to data type mismatch: argument 1 requires array type, however, '`pscore`' is of int type.;[0m
[0m[[0m[31merror[0m] [0m[0m'Filter (array_min(pscore#28) < 0.2)[0m
[0m[[0m[31merror[0m] [0m[0m+- Project [report#23.id AS id#25, report#23.emotion AS emotion#26, report#23.behavior AS behavior#27, report#23.pscore AS pscore#28, report#23.datetime AS datetime#29, report#23.lat AS lat#30, report#23.lon AS lon#31, report#23.words AS words#32][0m
[0m[[0m[31merror[0m] [0m[0m   +- Project [from_json(StructField(id,StringType,true), StructField(emotion,StringType,true), StructField(behavior,StringType,true), StructField(pscore,IntegerType,true), StructField(datetime,StringType,true), StructField(lat,StringType,true), StructField(lon,StringType,true), StructField(words,ArrayType(StringType,true),true), value#21, Some(Europe/Paris)) AS report#23][0m
[0m[[0m[31merror[0m] [0m[0m      +- Project [cast(value#8 as string) AS value#21][0m
[0m[[0m[31merror[0m] [0m[0m         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7a6e0f3c, org.apache.spark.sql.kafka010.KafkaSourceProvider, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@49c37918, [startingOffsets=latest, kafka.bootstrap.servers=localhost:9092, subscribe=test, failOnDataLoss=false], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@43de88f3,org.apache.spark.sql.kafka010.KafkaSourceProvider,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> test, startingOffsets -> latest, failOnDataLoss -> false),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6][0m
[0m[[0m[31merror[0m] [0m[0mTotal time: 7 s, completed Apr 3, 2022, 8:58:36 PM[0m
[0m[[0m[0mdebug[0m] [0m[0m> Exec(idea-shell, None, None)[0m
